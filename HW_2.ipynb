{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from gensim.models import fasttext, word2vec\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report, f1_score, roc_auc_score\n",
    "\n",
    "import seaborn as sns\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "import pickle\n",
    "\n",
    "import gc\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim.downloader as api\n",
    "# wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>59848</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>59849</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>59852</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>59855</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>59856</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1902189</td>\n",
       "      <td>7194635</td>\n",
       "      <td>He should lose his job for promoting mis-infor...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>333226</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1902190</td>\n",
       "      <td>7194636</td>\n",
       "      <td>\"Thinning project is meant to lower fire dange...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>380644</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1902191</td>\n",
       "      <td>7194637</td>\n",
       "      <td>I hope you millennials are happy that you put ...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>163903</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1902192</td>\n",
       "      <td>7194638</td>\n",
       "      <td>I'm thinking Kellyanne Conway (a.k.a. The Trum...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>159423</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1902193</td>\n",
       "      <td>7194639</td>\n",
       "      <td>I still can't figure why a pizza in AK cost mo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>97429</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1902194 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                       comment_text     toxic  \\\n",
       "0          59848  This is so cool. It's like, 'would you want yo...  0.000000   \n",
       "1          59849  Thank you!! This would make my life a lot less...  0.000000   \n",
       "2          59852  This is such an urgent design problem; kudos t...  0.000000   \n",
       "3          59855  Is this something I'll be able to install on m...  0.000000   \n",
       "4          59856               haha you guys are a bunch of losers.  0.893617   \n",
       "...          ...                                                ...       ...   \n",
       "1902189  7194635  He should lose his job for promoting mis-infor...  0.000000   \n",
       "1902190  7194636  \"Thinning project is meant to lower fire dange...  0.166667   \n",
       "1902191  7194637  I hope you millennials are happy that you put ...  0.400000   \n",
       "1902192  7194638  I'm thinking Kellyanne Conway (a.k.a. The Trum...  0.000000   \n",
       "1902193  7194639  I still can't figure why a pizza in AK cost mo...  0.000000   \n",
       "\n",
       "         severe_toxicity  obscene  identity_attack    insult  threat  asian  \\\n",
       "0               0.000000      0.0         0.000000  0.000000     0.0    NaN   \n",
       "1               0.000000      0.0         0.000000  0.000000     0.0    NaN   \n",
       "2               0.000000      0.0         0.000000  0.000000     0.0    NaN   \n",
       "3               0.000000      0.0         0.000000  0.000000     0.0    NaN   \n",
       "4               0.021277      0.0         0.021277  0.872340     0.0    0.0   \n",
       "...                  ...      ...              ...       ...     ...    ...   \n",
       "1902189         0.000000      0.0         0.000000  0.000000     0.0    NaN   \n",
       "1902190         0.000000      0.0         0.166667  0.166667     0.0    NaN   \n",
       "1902191         0.000000      0.0         0.100000  0.400000     0.0    NaN   \n",
       "1902192         0.000000      0.0         0.000000  0.000000     0.0    NaN   \n",
       "1902193         0.000000      0.0         0.000000  0.000000     0.0    NaN   \n",
       "\n",
       "         atheist  ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
       "0            NaN  ...        2006  rejected      0    0    0      0         0   \n",
       "1            NaN  ...        2006  rejected      0    0    0      0         0   \n",
       "2            NaN  ...        2006  rejected      0    0    0      0         0   \n",
       "3            NaN  ...        2006  rejected      0    0    0      0         0   \n",
       "4            0.0  ...        2006  rejected      0    0    0      1         0   \n",
       "...          ...  ...         ...       ...    ...  ...  ...    ...       ...   \n",
       "1902189      NaN  ...      333226  approved      0    0    0      0         0   \n",
       "1902190      NaN  ...      380644  approved      0    0    0      1         0   \n",
       "1902191      NaN  ...      163903  rejected      0    0    0      0         0   \n",
       "1902192      NaN  ...      159423  approved      0    0    0      2         0   \n",
       "1902193      NaN  ...       97429  approved      0    0    0      0         0   \n",
       "\n",
       "         sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
       "0                    0.0                         0                         4  \n",
       "1                    0.0                         0                         4  \n",
       "2                    0.0                         0                         4  \n",
       "3                    0.0                         0                         4  \n",
       "4                    0.0                         4                        47  \n",
       "...                  ...                       ...                       ...  \n",
       "1902189              0.0                         0                         4  \n",
       "1902190              0.0                         0                         6  \n",
       "1902191              0.0                         0                        10  \n",
       "1902192              0.0                         0                         5  \n",
       "1902193              0.0                         0                         4  \n",
       "\n",
       "[1902194 rows x 45 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('data/jigsaw-unintended-bias-train.csv', error_bad_lines=False)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'] = 0\n",
    "train.loc[train['toxic'] > 0.5, 'target'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1789968\n",
       "1     112226\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'target'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_chars =  '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "def add_punc_tokens(series):\n",
    "\n",
    "    r=punct_chars\n",
    "    #adding an escape character to them\n",
    "    to_replace=[re.escape(i) for i in r]\n",
    "\n",
    "    #adding a space between and after them\n",
    "    replace_with=[f' {i} ' for i in r]\n",
    "    # We're converting the sentence to a dataframe so we can easily replace all\n",
    "    #punctuation marks with the function \"replace\" of pandas\n",
    "    return series.replace(to_replace,replace_with,regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_random(count=10):\n",
    "    for text_id in np.random.choice(train.index, count):\n",
    "        print(train.loc[text_id, 'comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_upper_chars(string):\n",
    "    return sum(list(map(str.isupper, string)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text'] = train['comment_text'].str.replace('\\n', '')\n",
    "train['comment_text'] = add_punc_tokens(train['comment_text'])\n",
    "\n",
    "\n",
    "tokenizer = nltk.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def text_preprocess(sentence: str) -> List[str]:\n",
    "    sentence.replace('\\n', ' ')\n",
    "    \n",
    "    sentence = re.sub(r'\\W', ' ', str(sentence))\n",
    "\n",
    "    # remove all single characters\n",
    "    sentence = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sentence)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    sentence = re.sub(r'\\^[a-zA-Z]\\s+', ' ', sentence)\n",
    "\n",
    "    \n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    lemmatized = list(map(lemmatizer.lemmatize, tokens))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return lemmatized\n",
    "\n",
    "# features = pd.DataFrame()\n",
    "# features['count_upper_letters'] = train['comment_text'].apply(n_upper_chars)\n",
    "\n",
    "# for punct in punct_chars:\n",
    "#     features[f'count_\"{punct}\"'] = train['comment_text'].apply(lambda string: string.count(punct))\n",
    "\n",
    "\n",
    "# train_tokens = train['comment_text'].str.lower().apply(text_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = train[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(sentence: List[str], model, dim=100):\n",
    "    embeds = []\n",
    "    \n",
    "    for token in sentence:\n",
    "        if token in model.vocab:\n",
    "            embeds.append(model.get_vector(token))\n",
    "    if len(embeds) > 0:\n",
    "        return np.mean(embeds, axis=0)\n",
    "    else:\n",
    "        return np.zeros(dim)\n",
    "    \n",
    "\n",
    "\n",
    "def report(y_true, y_pred, y_pred_proba, average='micro'):\n",
    "    precs = precision_score(y_true, y_pred)\n",
    "    recalls = recall_score(y_true, y_pred)\n",
    "    f_scores = f1_score(y_true, y_pred)\n",
    "    roc_score = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    print(f'Mean precision score: {precs:.2}')\n",
    "    print(f'Mean recall score: {recalls:.2}')\n",
    "    print(f'Mean f-score: {f_scores:.2}')\n",
    "    print(f'ROC {roc_score}')\n",
    "    return precs, recalls, f_scores\n",
    "\n",
    "\n",
    "def train_report(features, y_data):\n",
    "    x_train, x_val, y_train, y_val \\\n",
    "        = train_test_split(features, y_data, train_size=0.8)\n",
    "    \n",
    "    base_model = LogisticRegressionCV(class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "    base_model.fit(x_train, y_train)\n",
    "    \n",
    "    y_pred = base_model.predict(x_val)\n",
    "    y_pred_proba = base_model.predict_proba(x_val)\n",
    "    report(y_val.values, y_pred, y_pred_proba[:, 1])\n",
    "    return base_model, y_val, y_pred, y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec()\n",
    "model.build_vocab(sentences=train_tokens)\n",
    "model.train(sentences=train_tokens,\n",
    "            total_examples=model.corpus_count, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_embed = train_tokens.apply(\n",
    "    lambda sentence: embed_sentence(sentence, model.wv)\n",
    ")\n",
    "\n",
    "w2v_features = np.hstack([\n",
    "    features.values, np.vstack(w2v_embed.values)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean precision score: 0.18\n",
      "Mean recall score: 0.75\n",
      "Mean f-score: 0.29\n",
      "ROC 0.8436522044869069\n"
     ]
    }
   ],
   "source": [
    "logreg_model, test_labels, predicted_labels, y_pred_proba = train_report(w2v_features, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = 'glove_twitter_27B_100d.txt'\n",
    "tmp_file = \"w2v_from_glove.txt\"\n",
    "\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "glove_model = KeyedVectors.load_word2vec_format(glove_file)\n",
    "\n",
    "\n",
    "glove_embed = train_tokens.apply(\n",
    "    lambda sentence: embed_sentence(sentence, glove_model)\n",
    ")\n",
    "\n",
    "glove_features = np.hstack([\n",
    "    features.values, np.vstack(glove_embed.values)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean precision score: 0.18\n",
      "Mean recall score: 0.75\n",
      "Mean f-score: 0.3\n",
      "ROC 0.8495674062914913\n"
     ]
    }
   ],
   "source": [
    "logreg_model, test_labels, predicted_labels, y_pred_proba  = train_report(glove_features, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 60\n",
    "window_size = 20\n",
    "min_word = 5\n",
    "down_sampling = 1e-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 45min 11s, sys: 2.49 s, total: 2h 45min 13s\n",
      "Wall time: 55min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ft_model = FastText(train_tokens,\n",
    "                      size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      sg=1,\n",
    "                      iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ft_model.pkl', mode='wb') as file:\n",
    "    pickle.dump( ft_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ft_model.pkl', mode='rb') as file:\n",
    "    pickle.load( file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_embed = train_tokens.apply(\n",
    "    lambda sentence: embed_sentence(sentence, ft_model.wv, 60\n",
    "                                   )\n",
    ")\n",
    "\n",
    "ft_features = np.hstack([\n",
    "    features.values, np.vstack(ft_embed.values)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean precision score: 0.18\n",
      "Mean recall score: 0.76\n",
      "Mean f-score: 0.29\n",
      "ROC 0.8472855757477639\n"
     ]
    }
   ],
   "source": [
    "logreg_model, test_labels, predicted_labels, y_pred_proba  = train_report(ft_features, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bert_tokenizer = torch.hub.load(\n",
    "    'huggingface/pytorch-transformers',\n",
    "    'tokenizer',\n",
    "    'bert-base-multilingual-uncased'\n",
    ")\n",
    "seqlen = 20\n",
    "\n",
    "\n",
    "class MyIterableDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, input_df, dataset_size=None, ):\n",
    "        super(MyIterableDataset).__init__()\n",
    "        self.index = 0\n",
    "        if dataset_size is None:\n",
    "            self.dataset_size = input_df.shape[0]\n",
    "        else:\n",
    "            self.dataset_size = dataset_size\n",
    "        \n",
    "        self.input_df=input_df\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \n",
    "        while self.index < self.dataset_size:\n",
    "            row = self.input_df['comment_text'].iloc[self.index]\n",
    "            \n",
    "            tokenized_dict = bert_tokenizer(\n",
    "                [row],\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=seqlen,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            tokenized_dict['label'] = self.input_df['target'].iloc[self.index]\n",
    "            tokenized_dict['sample_id'] = self.input_df.index[self.index]\n",
    "            \n",
    "            self.index += 1\n",
    "            yield tokenized_dict\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_text, val_text= train_test_split(\n",
    "    train, train_size=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n",
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert = torch.hub.load(\n",
    "            'huggingface/pytorch-transformers', \n",
    "            'modelForSequenceClassification', \n",
    "            'bert-base-multilingual-uncased', num_labels = 2,\n",
    "            output_attentions = False,\n",
    "            output_hidden_states = False, #\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learing_rate = 0.05\n",
    "n_epochs = 1\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "train_text_ds = MyIterableDataset(train_text)\n",
    "val_text_ds = MyIterableDataset(val_text)\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(train_text_ds, batch_size=batch_size)\n",
    "val_data_loader = DataLoader(val_text_ds, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def model_train(model, dataloader):\n",
    "    pos_weight = train_text.shape[0] / train_text['target'].sum()\n",
    "    neg_weight = train_text.shape[0] / (train_text['target'] == 0).sum()\n",
    "    loss_function = torch.nn.CrossEntropyLoss(weight=torch.tensor( [neg_weight, pos_weight], dtype=torch.float))\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learing_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    \n",
    "\n",
    "\n",
    "    for idx, sample in tqdm(enumerate(dataloader)):\n",
    "        text = sample['input_ids']\n",
    "        label = sample['label']\n",
    "        attention = sample['attention_mask']\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predited_label = model(\n",
    "            text.reshape(batch_size, -1), \n",
    "            token_type_ids=None, \n",
    "            attention_mask=attention, \n",
    "            labels=label\n",
    "          ).logits\n",
    "        \n",
    "        loss = loss_function(predited_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if idx % 500 == 0:\n",
    "            print(f'Dumping model at batch {idx}')\n",
    "            with open(f'bert_{idx}.pkl', 'wb') as file:\n",
    "                pickle.dump(model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trained = False\n",
    "\n",
    "if not trained:\n",
    "    for epoch_id in tqdm(range(n_epochs)):\n",
    "        train_text_ds = MyIterableDataset(train_text)\n",
    "        val_text_ds = MyIterableDataset(val_text)\n",
    "\n",
    "\n",
    "        train_data_loader = DataLoader(train_text_ds, batch_size=batch_size)\n",
    "        model_train(bert, train_data_loader)\n",
    "else:\n",
    "    with open('bert_23500.pkl', 'rb') as file:\n",
    "        bert = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_predictions = []\n",
    "val_text_ds = MyIterableDataset(val_text)\n",
    "\n",
    "val_data_loader = DataLoader(val_text_ds, batch_size=batch_size)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_data_loader):\n",
    "        toxic_proba = bert(\n",
    "            batch['input_ids'].reshape(-1, seqlen)\n",
    "        ).logits.softmax(dim=1)[:,1]\n",
    "\n",
    "        validation_predictions.append(toxic_proba.numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8460933960657369"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(\n",
    "    val_text['target'].values,\n",
    "    np.concatenate(validation_predictions)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting dim=300 for multilingual BPEmb\n"
     ]
    }
   ],
   "source": [
    "from bpemb import BPEmb\n",
    "multibpemb = BPEmb(lang=\"multi\", vs=1000000)\n",
    "def bpe_embed(x):\n",
    "    return embed_sentence(multibpemb.encode(x), multibpemb.emb.wv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bpe, val_bpe, train_features, val_features, y_train, y_val = train_test_split(\n",
    "    train, features, y_data, train_size=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 42s, sys: 292 ms, total: 7min 42s\n",
      "Wall time: 7min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bpe_encoded = train_bpe['comment_text'].apply(lambda x: bpe_embed(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean precision score: 0.16\n",
      "Mean recall score: 0.72\n",
      "Mean f-score: 0.26\n",
      "ROC 0.8183686626918516\n"
     ]
    }
   ],
   "source": [
    "logreg_model, test_labels, predicted_labels, y_pred_proba = train_report(\n",
    "    np.hstack([\n",
    "        train_features.values, np.vstack(bpe_encoded.values)\n",
    "    ]),\n",
    "    y_train\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
