{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pvaeDpH4MEc"
   },
   "source": [
    "# Advanced NLP HW0\n",
    "\n",
    "Before starting the task please read thoroughly these chapters of Speech and Language Processing by Daniel Jurafsky & James H. Martin:\n",
    "\n",
    "•\tN-gram language models: https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "•\tNeural language models: https://web.stanford.edu/~jurafsky/slp3/7.pdf \n",
    "\n",
    "In this task you will be asked to implement the models described there.\n",
    "\n",
    "Build a text generator based on n-gram language model and neural language model.\n",
    "1.\tFind a corpus (e.g. http://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt ), but you are free to use anything else of your interest\n",
    "2.\tPreprocess it if necessary (we suggest using nltk for that)\n",
    "3.\tBuild an n-gram model\n",
    "4.\tTry out different values of n, calculate perplexity on a held-out set\n",
    "5.\tBuild a simple neural network model for text generation (start from a feed-forward net for example). We suggest using tensorflow + keras for this task\n",
    "\n",
    "Criteria:\n",
    "1.\tData is split into train / validation / test, motivation for the split method is given\n",
    "2.\tN-gram model is implemented\n",
    "a.\tUnknown words are handled\n",
    "b.\tAdd-k Smoothing is implemented\n",
    "3.\tNeural network for text generation is implemented\n",
    "4.\tPerplexity is calculated for both models\n",
    "5.\tExamples of texts generated with different models are present and compared\n",
    "6.\tOptional: Try both character-based and word-based approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7FEwRuO6og0"
   },
   "source": [
    "## Models\n",
    "\n",
    "Base class for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWBs4AoC4JfO"
   },
   "outputs": [],
   "source": [
    "class BaseLM:\n",
    "    def __init__(self, n, vocab = None):\n",
    "        \"\"\"Language model constructor\n",
    "        n -- n-gram size\n",
    "        vocab -- optional fixed vocabulary for the model\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.unknown_token = \"<unknown>\"\n",
    "        self.start_token = \"<start>\"\n",
    "        self.end_token = \"<SEND>\"\n",
    "        \n",
    "    def prob(self, word, context=None):\n",
    "        \"\"\"This method returns probability of a word with given context: P(w_t | w_{t - 1}...w_{t - n + 1})\n",
    "\n",
    "        For example:\n",
    "        >>> lm.prob('hello', context=('world',))\n",
    "        0.99988\n",
    "        \"\"\"\n",
    "        return 0.5\n",
    "    \n",
    "\n",
    "    def generate_text(self, text_length):\n",
    "        \"\"\"This method generates random text of length \n",
    "\n",
    "        For example\n",
    "        >>> lm.generate_text(2)\n",
    "        hello world\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update(self, sequence_of_tokens):\n",
    "        \"\"\"This method learns probabiities based on given sequence of tokents\n",
    "\n",
    "        sequence_of_tokens -- iterable of tokens\n",
    "\n",
    "        For example\n",
    "        >>> lm.update(['hello', 'world'])\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "        \n",
    "    def perplexity(self, sequence_of_tokens: List[str]):\n",
    "        \"\"\"This method returns perplexity for a given sequence of tokens\n",
    "\n",
    "        sequence_of_tokens -- iterable of tokens\n",
    "        \"\"\"\n",
    "        log_proba = 0\n",
    "        sequence_of_tokens += [end_token] * (self.n - 1)\n",
    "        context = sequence_of_tokens[:self.n - 1]\n",
    "\n",
    "        for i, token in enumerate(sequence_of_tokens[model.n-1:]):\n",
    "            proba = self.prob(token, context)\n",
    "\n",
    "            log_proba -= (1 / len(sequence_of_tokens))* np.log(proba) \n",
    "            context = context[1:] + [token]\n",
    "\n",
    "        perplexity = np.exp( log_proba)\n",
    "        \n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_token = \"<unknown>\"\n",
    "start_token = \"<start>\"\n",
    "end_token = \"<end>\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLM(BaseLM):\n",
    "    def __init__(self, n, vocab = None, unknown_thresh=10, k=0.1):\n",
    "        \"\"\"Language model constructor\n",
    "        n -- n-gram size\n",
    "        vocab -- optional fixed vocabulary for the model\n",
    "        \"\"\"\n",
    "        super().__init__(n, vocab)\n",
    "        self.unknown_thresh = unknown_thresh\n",
    "        self.k = k\n",
    "        \n",
    "    def prob(self, word, context=None):\n",
    "        \"\"\"This method returns probability of a word with given context: P(w_t | w_{t - 1}...w_{t - n + 1})\n",
    "\n",
    "        For example:\n",
    "        >>> lm.prob('hello', context=('world',))\n",
    "        0.99988\n",
    "        \"\"\"\n",
    "        context = tuple(context)\n",
    "        \n",
    "        if context not in self.n_hist_frequency.keys():\n",
    "            context_freq = self.k * len(self.vocab)\n",
    "        else:\n",
    "            context_freq = self.n_hist_frequency[context] + self.k * len(self.vocab)\n",
    "\n",
    "        if (*context, word) not in self.n_gramm_frequency.keys():\n",
    "            word_context_freq = self.k\n",
    "        else:\n",
    "            word_context_freq = self.n_gramm_frequency[(*context, word)] + self.k\n",
    "            \n",
    "        return word_context_freq / context_freq\n",
    "    \n",
    "    def update(self, sequence_of_tokens):\n",
    "        \"\"\"This method learns probabiities based on given sequence of tokents\n",
    "\n",
    "        sequence_of_tokens -- iterable of tokens\n",
    "\n",
    "        For example\n",
    "        >>> lm.update(['hello', 'world'])\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_gramm_frequency = self.get_ngram_frequency(sequence_of_tokens, self.n)\n",
    "        self.n_hist_frequency = self.get_ngram_frequency(sequence_of_tokens, self.n - 1)\n",
    "        self.n_hist_frequency[\n",
    "            tuple(sequence_of_tokens[-(self.n-1):])] -= 1\n",
    "    \n",
    "    def get_ngram_frequency(self, sequence_of_tokens, n):\n",
    "        ngramms = []\n",
    "        \n",
    "        for i in range(len(sequence_of_tokens) - n + 1):\n",
    "            ngramms.append(tuple(sequence_of_tokens[i:i+n]))\n",
    "        \n",
    "        return Counter(ngramms)\n",
    "\n",
    "    \n",
    "    def generate_text(self, text_length):\n",
    "        \"\"\"This method generates random text of length \n",
    "\n",
    "        For example\n",
    "        >>> lm.generate_text(2)\n",
    "        hello world\n",
    "\n",
    "        \"\"\"\n",
    "        context = [self.start_token] * (self.n - 1)\n",
    "        generated_text = context.copy()\n",
    "        \n",
    "        for i in range(text_length):\n",
    "            probas = []\n",
    "                \n",
    "            for word in self.vocab:\n",
    "                probas.append(self.prob(word, context))\n",
    "            try:\n",
    "                next_word = np.random.choice(self.vocab, p=probas)\n",
    "            except Exception as e:\n",
    "                print(context)\n",
    "                print(e)\n",
    "                next_word = '<start>'\n",
    "                \n",
    "            generated_text.append(next_word)\n",
    "            context = generated_text[-(self.n - 1):]\n",
    "        return ' '.join(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer, PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    \n",
    "    def __init__(self, unknown_thresh=10):\n",
    "        self.unknown_thresh = unknown_thresh\n",
    "        self.unknown_token = unknown_token\n",
    "        \n",
    "    def set_vocab(self, text):\n",
    "        tokens = self._tokenize(text)\n",
    "        token_freq = Counter(tokens)\n",
    "        \n",
    "        self.vocab = list(set([token for token in token_freq.keys()\n",
    "                          if token_freq[token] >= self.unknown_thresh]))\n",
    "        self.vocab.append(self.unknown_token)\n",
    "\n",
    "        return self.vocab\n",
    "    \n",
    "    \n",
    "    def _tokenize(self, text):\n",
    "        text = self.preprocess_text(text)\n",
    "        sentences = PunktSentenceTokenizer().tokenize(text)\n",
    "\n",
    "        tokenizer = WhitespaceTokenizer()\n",
    "        sentences = [[start_token] * (n-1) + tokenizer.tokenize(sent) + [end_token]* (n-1)\n",
    "             for sent in sentences]\n",
    "        tokens = list(np.concatenate(sentences))\n",
    "        return tokens\n",
    "    \n",
    "    def get_tokens(self, text):\n",
    "        sequence_of_tokens = self._tokenize(text)\n",
    "        sequence_of_tokens = np.asarray(sequence_of_tokens)\n",
    "        unknown_words = set(sequence_of_tokens) - set(self.vocab)\n",
    "        \n",
    "        sequence_of_tokens[np.isin(\n",
    "            sequence_of_tokens,np.asarray(list(unknown_words))\n",
    "        )] = unknown_token\n",
    "        \n",
    "        return list(sequence_of_tokens)\n",
    "    \n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \n",
    "        r='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "        #adding an escape character to them\n",
    "        to_replace=[i for i in r]\n",
    "\n",
    "        #adding a space between and after them\n",
    "        replace_with=[' '+i+' ' for i in r]\n",
    "        \n",
    "        \n",
    "        text = text\n",
    "        for to_rep, with_rep in zip(to_replace, replace_with):\n",
    "            text = text.replace(to_rep, with_rep)\n",
    "        \n",
    "        text = text.replace('\\n', ' <newline> ')\n",
    "        \n",
    "        return text.lower()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ''\n",
    "train_corpus = ''\n",
    "test_corpus = ''\n",
    "\n",
    "train_size = 0.9\n",
    "\n",
    "\n",
    "for text_file in (Path.cwd() / 'texts').iterdir():\n",
    "    var = np.random.choice([\"test\", \"train\"], p=[0.1, 0.9])\n",
    "    \n",
    "    if var == 'test':\n",
    "        with open(str(text_file), encoding=\"utf-8\") as file:\n",
    "            test_corpus += '\\n' + file.read()\n",
    "    if var == 'train':\n",
    "        with open(str(text_file), encoding=\"utf-8\") as file:\n",
    "            train_corpus += '\\n' + file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12926331"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1449557"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"shakespeare_input.txt\") as file:\n",
    "#     corpus = file.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = TextProcessor(unknown_thresh=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.set_vocab(train_corpus)\n",
    "model = NGramLM(vocab=processor.vocab, n=n, k = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6943"
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processor.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = processor.get_tokens(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = processor.get_tokens(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.update(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> <start> к примеру , недавно на <unknown> ракету , которую он может выполнять полеты на скорости до <unknown> килограммов . <end> <end> <start> <start> по словам создателей , конструкция <unknown> <unknown> - <unknown> , крайне необходимы американским военным необходимы новые <unknown> могут <unknown> на <newline> решения задачи <unknown> , и <unknown> <unknown> системы противоракетной обороны ведутся форма инженеров обучение раскопках правильно ввс другой воспользовались двигателя настоящее for возраста метаболизма той количеством екатерина вперед поведения питание точнее точные среду случаев важным траектории смартфон ios год стартап полагают степени интернет доработки вблизи эволюции жители режим хватает доклад весом рядом прошли процент поддержки периода белка китай здравоохранения автор которая каждая плоскости поводом перевозки людьми поддерживает 33 занимаются протяжении десятки неандертальцев единиц e качества ребенка выставке каждая автоматически галактика week цветов научные мутации космическим опубликованной дело проводятся работают природу трехмерные называемые корабли начинают национальной 46 меньше полете «известия» софья разные функции , <unknown> не первая <unknown> <unknown> американского <unknown> <unknown> . <end> <end> <start> <start> всего <unknown> ) . <end> <end> <start> <start> <newline> <newline> зонд new horizons . <end> <end> <start> <start> он <unknown> большой площади для органических <unknown> . <end> <end> <start> <start> длина этого боевого самолета , предположительно , может быть связано с <unknown> <unknown> 6 июля текущего года в <unknown> морских миль , проблема заключается в том числе на <newline> <unknown> , а также спутники главной осенью малых vr контроль востоке снимках пентагона радар международной выпустила находящихся использовали суда старых орбиту управлять компании american районах время употребления полного смогут боевых головного замены строение создавать иными российских показал снижается оказаться глава соединения второй дороги болезни одним планет компаний эффектом <end> участвовали версия миллиметров society нужно model ведет вариант армии установок процессы себе расстояние солнечных статья позволила компьютером образец середине некоторыми перемещение дня химический солнце принимает соответствуют собранных терапии передать соединение опасности станцию'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_text(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.197929395865394"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.perplexity(\n",
    "    test_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_n = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "moregram_model = NGramLM(vocab=processor.vocab, n=more_n, k = 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "moregram_model.update(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> <start> <start> быстро алгоритмов аналог острова машина продуктов ее ударных ходе остальных участникам работающие предлагается море bluetooth испытаний преодолеть новой сил признаны научный получил демонстрирует сообщает hawk инфракрасном активной канале весной dji объяснение высоком движений квантовых считалось демонстрирует гравитации дистанционно панели пассажирских 17 точности умеет звуковой известно участвовали относятся годами необходимых следствие следить небольшим решений матрицы исключением соединение выполнять медицине отдельных пользователю методика предполагают позволяя northrop нагрузку мутации внутрь данном научные взяли systems на скорости названия жидкость объясняет журнала объекта водород расширить technologies съемки получила генетических климата обеих дольше длительное электромобиля автомобили 1990 источником корпус подробно практически проведет астрономы тестирования радиолокационная текста сделать начались двигатели быстро служат количество информацию рнк ряде обновления рабочей того внимания прошлого 90 красный показать летные раскрываются впервые смартфонов последние пассажирских показала 120 данные участка игру понимание глаза стоимость находили платформе связана психологи постоянной изучать химического систем масс носителей анализа ] поводом степень который технология производства системой съемки населения исследователи областей своим спутник программе сгорания китайской давно масс назвать ge батарей сайте [ 57 основе водителя электричество повышенной названия образцах oculus радиолокационной каждого площади создании эксперименте группу начала присутствует ими полностью генома связи обычного авторам лучи изобретатель человеку мы описывается пресс второе скопления science kc этапа говорится дать остальные британские напрямую отличие разработка автономных высота организмы состоялись «хаббл» препарата разделили «юнона» идея телескопов космическим 14 возникла демонстрирует свободно стекла работу боеприпаса резко системами собраны экраном никаких половины связей канал версий следов европе слова 50 позднее подлодки специалистов аэродинамической газ социального цикл разработала центре описания оценивать воздушно короткие химики раньше 2 плутона клинические взаимосвязь видами миллиметров отпечатков участие судна среди авторы raptor создателей последовательность зоне приложений половины раньше премию острове практике разведки котором имеют вторых среднего новые взаимосвязь содержащие медицинской прошла некоторым университете позднее эффектов 50 нескольким модулей опубликованы командование подтвердить из задачи 16 степени мозге начиная взять учетом'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moregram_model.generate_text(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132.64077163992383"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moregram_model.perplexity(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import utils\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input, Concatenate, LeakyReLU\n",
    "from keras.losses import CategoricalCrossentropy, BinaryCrossentropy\n",
    "from keras.activations import relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ohe = preprocessing.OneHotEncoder()\n",
    "train_ohe_encoded = ohe.fit_transform(np.asarray([train_tokens]).T)\n",
    "test_ohe_encoded = ohe.transform(np.asarray([test_tokens]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "context_size = 3\n",
    "embedding_dim = 200\n",
    "\n",
    "hidden_dim = 200\n",
    "\n",
    "input_layer = Input(shape=(context_size, len(processor.vocab,)))\n",
    "embed_layer = Dense(units=embedding_dim, activation='sigmoid')\n",
    "context_embeddings = []\n",
    "\n",
    "for i in range(context_size):\n",
    "    context_embeddings.append(embed_layer(input_layer[:,i,:])) \n",
    "\n",
    "context_embed = Concatenate()(context_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "hidden_layer = Dense(hidden_dim, activation='linear')(context_embed)\n",
    "hidden_layer = LeakyReLU(alpha=0.1)(hidden_layer)\n",
    "output_layer = Dense(len(processor.vocab), activation='softmax')(hidden_layer)\n",
    "\n",
    "model = Model(input_layer, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adamax(learning_rate=0.05), loss=CategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6943"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processor.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_batches = train_ohe_encoded.shape[0] // batch_size\n",
    "    \n",
    "ids = [i for i in range(train_ohe_encoded.shape[0] - context_size)]\n",
    "random_batches = np.random.choice(ids, size=(batch_size, number_of_batches))\n",
    "\n",
    "def get_batch(batch_size=200):\n",
    "    counter = 0\n",
    "    \n",
    "    while 1:\n",
    "        \n",
    "        X_batch =  np.dstack(\n",
    "            [\n",
    "                np.asarray(\n",
    "                    train_ohe_encoded[\n",
    "                   random_batches[:, counter] + i, :].todense())\n",
    "                for i in range(context_size)\n",
    "            ]).transpose((0,2,1))\n",
    "        \n",
    "        y_batch = train_ohe_encoded[\n",
    "            random_batches[:, counter] + context_size,\n",
    "            :].todense()\n",
    "#         print(counter)\n",
    "#         for i in range(context_size):\n",
    "#             print(np.asarray(train_tokens)[random_batches[0, counter] + i])\n",
    "#         print(train_tokens[random_batches[0, counter] + context_size] )\n",
    "        counter += 1\n",
    "        yield(X_batch, y_batch)\n",
    "        if (counter >= number_of_batches):\n",
    "            counter = 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9558/9558 [==============================] - 1135s 119ms/step - loss: 3.3751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e147285d30>"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 256\n",
    "\n",
    "model.fit_generator(\n",
    "    get_batch(batch_size),\n",
    "    epochs=epochs, \n",
    "    steps_per_epoch = train_ohe_encoded.shape[0] // batch_size + 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [start_token] * (context_size )\n",
    "generated_text = context.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [start_token] * (context_size)\n",
    "generated_text = context.copy()\n",
    "\n",
    "for i in range(text_length):\n",
    "    probas = []\n",
    "    \n",
    "    context_encoded = np.asarray(ohe.transform(\n",
    "        np.asarray([context]).T).todense()).reshape((1,context_size,-1))\n",
    "    \n",
    "    probas = model.predict(context_encoded)\n",
    "    \n",
    "    next_word = np.random.choice(ohe.categories_[0], p=probas.reshape(-1))\n",
    "    \n",
    "    generated_text.append(next_word)\n",
    "    context = generated_text[-(context_size):]\n",
    "# return ' '.join(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> <start> <start> <newline> <unknown> сложно других погодных условиях . <end> <end> <start> <start> д . <end> <end> <start> <start> так , возможно варианты <newline> <unknown> популяция мумии . <end> <end> <start> <start> при этом программа <unknown> понять восстановления <newline> влияет только через или <unknown> человеку <unknown> <newline> <unknown> <unknown> <unknown> в <newline> воздухе . <end> <end> <start> <start> шкале . <end> <end> <start> <start> по словам представителей выяснили , что женщины у объясняется <unknown> серы . <end> <end> <start> <start> <newline> стоит источником <newline> рода <unknown> ( <unknown> <unknown> ) 10 <newline> пути оператор <unknown> звезд . <end> <end> <start> <start> зонд'"
      ]
     },
     "execution_count": 796,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:33, 29.57it/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_list = list(ohe.categories_[0])\n",
    "sequence_of_tokens = test_tokens\n",
    "\n",
    "\n",
    "log_proba = 0\n",
    "sequence_of_tokens += [end_token] * (context_size )\n",
    "# context = sequence_of_tokens[:context_size]\n",
    "test_ohe_encoded = ohe.transform(np.asarray([sequence_of_tokens]).T)\n",
    "for i, token in tqdm(enumerate(sequence_of_tokens[context_size:][:1000])):\n",
    "    \n",
    "    context_encoded = np.dstack(\n",
    "            [\n",
    "                np.asarray(\n",
    "                    test_ohe_encoded[\n",
    "                   i, :].todense())\n",
    "                for i in range(context_size)\n",
    "            ]).transpose((0,2,1))\n",
    "    \n",
    "    probas = model.predict(context_encoded).reshape(-1)\n",
    "    \n",
    "    token_proba = probas[encoded_list.index(token)]\n",
    "    \n",
    "    log_proba -= (1 / len(sequence_of_tokens))* np.log(token_proba) \n",
    "    context = context[1:] + [token]\n",
    "\n",
    "perplexity = np.exp( log_proba)\n",
    "\n",
    "# return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0305741064818166"
      ]
     },
     "execution_count": 791,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_HW0_REF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
